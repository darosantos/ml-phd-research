{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JJEDRH8iz7a5"
   },
   "outputs": [],
   "source": [
    "FOLDER_ROOT = 'F:\\Github'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "czZKdvSrlj3A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#import matplotlib as mpl\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "import sys\n",
    "sys.path.append(FOLDER_ROOT)\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from os import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midas.v3.ktree import KTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ml-YZtmYl9rR"
   },
   "outputs": [],
   "source": [
    "df_stream = pd.read_csv('https://github.com/scikit-multiflow/streaming-datasets/raw/master/agr_a.csv',\n",
    "                        engine='c', low_memory=True, memory_map=True)\n",
    "\n",
    "X = df_stream[df_stream.columns[:-1]]\n",
    "y = df_stream['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zeZt3b_i0e9U"
   },
   "outputs": [],
   "source": [
    "class KTreeClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "        K Tree Classifier V3\n",
    "        strategy = ['auto', 'score']\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 ensemble_base,\n",
    "                 k_tree=25,\n",
    "                 strategy=\"auto\",\n",
    "                 metric_type=\"roc_auc_score\",\n",
    "                 metric_param={},\n",
    "                 voting=\"majority\",\n",
    "                 n_jobs=-1,\n",
    "                 verbose=0):\n",
    "        self.ensemble = ensemble_base\n",
    "        self.n_tree = k_tree\n",
    "        self.strategy = str(strategy).lower()\n",
    "        self.metric_type = str(metric_type).lower()\n",
    "        self.metric_param = metric_param\n",
    "        self.voting = str(voting).lower()\n",
    "        self.n_jobs = n_jobs\n",
    "        self.verbose = verbose\n",
    "        #\n",
    "        self.metric = None\n",
    "        self.args_metric = None\n",
    "        self.index_k_tree = []\n",
    "        self.fit_scores_ = []\n",
    "\n",
    "        if isinstance(k_tree, float):\n",
    "          self.n_tree = int(k_tree*self._get_ensemble_size())\n",
    "\n",
    "\n",
    "    def _get_n_jobs(self):\n",
    "        \"\"\"\n",
    "        Utility for to return get the number of CPUs in the system or the\n",
    "        number choose from user\n",
    "        \"\"\"\n",
    "        return (cpu_count() if self.n_jobs == -1 else self.n_jobs)\n",
    "    \n",
    "    def get_score_train(self, std=False):\n",
    "        \"\"\"\n",
    "            None\n",
    "        \"\"\"\n",
    "        if std:\n",
    "            return (np.mean(self.fit_scores_), np.std(self.fit_scores_))\n",
    "        return np.mean(self.fit_scores_)\n",
    "    \n",
    "    \n",
    "    def get_index_estimators(self):\n",
    "        \"\"\"\n",
    "            None\n",
    "        \"\"\"\n",
    "        return list(self.index_k_tree)\n",
    "    \n",
    "    \n",
    "    def get_nk_tree(self):\n",
    "        \"\"\"\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.n_tree\n",
    "    \n",
    "    \n",
    "    def _get_ensemble_size(self):\n",
    "        \"\"\"\n",
    "          Return of the numbers DTs in ensemble\n",
    "        \"\"\"\n",
    "        return len(self.ensemble.estimators_)\n",
    "    \n",
    "    def _get_metric(self):\n",
    "        \"\"\"\n",
    "            Decode the metric function to use for evaluable classifiers\n",
    "        \"\"\"\n",
    "        if self.metric_type == 'accuracy_score':\n",
    "            return accuracy_score\n",
    "        elif self.metric_type == 'cohen_kappa_score':\n",
    "            return cohen_kappa_score\n",
    "        elif self.metric_type == 'f1_score':\n",
    "            return f1_score\n",
    "        elif self.metric_type == 'balanced_accuracy_score':\n",
    "            return balanced_accuracy_score\n",
    "        elif self.metric_type == 'precision_score':\n",
    "            return precision_score\n",
    "        elif self.metric_type == 'recall_score':\n",
    "            return recall_score\n",
    "        elif self.metric_type == 'roc_auc_score':\n",
    "            return roc_auc_score\n",
    "        else:\n",
    "            raise Exception('The parameter metric_type can not empty or unknow')\n",
    "    \n",
    "    def _get_metric_params(self):\n",
    "        dict_params = {}\n",
    "        if self.metric_type == 'accuracy_score':\n",
    "            dict_params = {'normalize': self.metric_param.get('normalize', True),\n",
    "                           'sample_weight': self.metric_param.get('sample_weight', None)\n",
    "            }\n",
    "        elif self.metric_type == 'cohen_kappa_score':\n",
    "            dict_params = {'labels': self.metric_param.get('labels', None),\n",
    "                           'weights': self.metric_param.get('weights', None),\n",
    "                           'sample_weight': self.metric_param.get('sample_weight', None)\n",
    "            }\n",
    "        elif self.metric_type == 'f1_score':\n",
    "            dict_params = {'labels': self.metric_param.get('labels', None),\n",
    "                           'pos_label': self.metric_param.get('pos_label', 1),\n",
    "                           'average': self.metric_param.get('average', 'binary'),\n",
    "                           'sample_weight': self.metric_param.get('sample_weight', None),\n",
    "                           'zero_division': self.metric_param.get('zero_divisiont', 'warn'),\n",
    "            }\n",
    "        elif self.metric_type == 'balanced_accuracy_score':\n",
    "            dict_params = {'sample_weigh': self.metric_param.get('sample_weigh', None),\n",
    "                           'adjusted': self.metric_param.get('adjusted', False)\n",
    "            }\n",
    "        elif self.metric_type == 'precision_score':\n",
    "            dict_params = {'labels': self.metric_param.get('labels', None),\n",
    "                           'pos_label': self.metric_param.get('pos_label', 1),\n",
    "                           'average': self.metric_param.get('average', 'binary'),\n",
    "                           'sample_weight': self.metric_param.get('sample_weight', None),\n",
    "                           'zero_division': self.metric_param.get('zero_division', 'warn')\n",
    "            }\n",
    "        elif self.metric_type == 'recall_score':\n",
    "            dict_params = {'labels': self.metric_param.get('labels', None),\n",
    "                           'pos_label': self.metric_param.get('pos_label', 1),\n",
    "                           'average': self.metric_param.get('average', 'binary'),\n",
    "                           'sample_weight': self.metric_param.get('sample_weight', None),\n",
    "                           'zero_division': self.metric_param.get('zero_division', 'warn')\n",
    "            }\n",
    "        elif self.metric_type == 'roc_auc_score':\n",
    "            dict_params = {'average': self.metric_param.get('average', 'macro'),\n",
    "                           'sample_weight': self.metric_param.get('sample_weight', None),\n",
    "                           'max_fpr': self.metric_param.get('max_fpr', None),\n",
    "                           'multi_class': self.metric_param.get('multi_class', 'raise'),\n",
    "                           'labels': self.metric_param.get('labels', None),\n",
    "            }\n",
    "        else:\n",
    "            raise Exception('Without a known metric_type parameter we cannot determine the other auxiliary parameters')\n",
    "            \n",
    "        return dict_params\n",
    "\n",
    "\n",
    "    def _parallel_predict(self, est, X, check_input=True):\n",
    "        \"\"\"\n",
    "        @Todo Validar 'est' para verificar se é um classificador e se tem o\n",
    "        método predict\n",
    "        \"\"\"\n",
    "        y_pred = Parallel(n_jobs=self._get_n_jobs(),\n",
    "                      verbose=self.verbose, \n",
    "                      require='sharedmem'\n",
    "        )(\n",
    "            delayed(est.predict)(Xi, check_input) \n",
    "            for Xi in np.array_split(X, self._get_n_jobs())\n",
    "        )\n",
    "\n",
    "        return np.ravel(y_pred)\n",
    "    \n",
    "    def _metric_evaluable(self, index, est, X, y_true, check_input=True):\n",
    "        \"\"\"\n",
    "            Return scored for X from metric with y_true\n",
    "        \"\"\"\n",
    "        y_pred = self._parallel_predict(est, X, check_input)\n",
    "        score = self.metric(y_true, y_pred, **self.args_metric)\n",
    "\n",
    "        return (index, score)\n",
    "    \n",
    "    def fit(self, X, y, check_input=True):\n",
    "        \"\"\"\n",
    "            X -> list of features values\n",
    "            y -> label for each X(i)\n",
    "        \"\"\"\n",
    "        if len(self.index_k_tree) != 0:\n",
    "              raise Exception(\"Classifier is fitted!\")\n",
    "    \n",
    "        if self.strategy == 'score':\n",
    "            self._fit_score(X, y, check_input)\n",
    "        elif self.strategy == 'auto':\n",
    "            pass\n",
    "        else:\n",
    "            raise('Unknow strategy!')\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _fit_score(self, X_true, y_true, check_input=True):\n",
    "        \"\"\"\n",
    "            X_true -> \n",
    "            y_true ->\n",
    "        \"\"\"\n",
    "        self.metric = self._get_metric()\n",
    "        self.args_metric = self._get_metric_params()\n",
    "        \n",
    "        lst_scores = Parallel(n_jobs=self.n_jobs,\n",
    "                              verbose=self.verbose, \n",
    "                              require='sharedmem'\n",
    "        )(\n",
    "            delayed(self._metric_evaluable)(xi, est, X_true, y_true, check_input) \n",
    "            for xi, est in enumerate(self.ensemble.estimators_)\n",
    "        )\n",
    "        \n",
    "        ordered_scores = sorted(lst_scores,\n",
    "                                key=lambda position: position[1], reverse=True)\n",
    "\n",
    "        self.index_k_tree = np.array([ik[0] for ik in ordered_scores[:self.n_tree]],\n",
    "                                     dtype=np.int64)\n",
    "\n",
    "        self.fit_scores_ = np.array([ik[1] for ik in ordered_scores[:self.n_tree]],\n",
    "                                    dtype=np.float64)\n",
    "        \n",
    "        del lst_scores, ordered_scores\n",
    "    \n",
    "    def fit_update(self, X, y, check_input=True):\n",
    "        \"\"\"\n",
    "            X -> list of features values\n",
    "            y -> label for each X(i)\n",
    "        \"\"\"\n",
    "        if self.index_k_tree.sum() == 0:\n",
    "            raise Exception(\"Classifier isn't fitted\")\n",
    "        \n",
    "        \n",
    "        if self.strategy == 'score':\n",
    "            self._fit_score(X, y, check_input)\n",
    "        elif self.strategy == 'auto':\n",
    "            pass\n",
    "        else:\n",
    "            raise('Unknow strategy!')\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def _fit_auto(self):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X, check_input=True):\n",
    "        \"\"\"\n",
    "            X -> unknow instances\n",
    "        \"\"\"\n",
    "        if len(self.index_k_tree) == 0:\n",
    "              raise Exception(\"Classifier not is fitted!\")\n",
    "                \n",
    "        if self.voting == \"majority\":\n",
    "            return self._predict_majority(X, check_input)\n",
    "        else:\n",
    "            raise Exception('Method of voting required')\n",
    "    \n",
    "    \n",
    "    def _predict_majority(self, X, check_input=True):\n",
    "        \"\"\"\n",
    "            X -> unknow instances\n",
    "        \"\"\"\n",
    "        predictions = Parallel(n_jobs=self.n_jobs,\n",
    "                               verbose=self.verbose,\n",
    "                               require='sharedmem'\n",
    "        )(\n",
    "          delayed(self._parallel_predict)(self.ensemble.estimators_[Ei], X, check_input)\n",
    "          for Ei in self.index_k_tree\n",
    "        )\n",
    "      \n",
    "        predictions = np.array(predictions, dtype=np.int64)\n",
    "        predictions = predictions.T\n",
    "          \n",
    "        maj = np.apply_along_axis(lambda x: np.bincount(x).argmax(), \n",
    "                                  axis=1, arr=predictions)\n",
    "        \n",
    "        del predictions\n",
    "        return maj\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "            @todo implement sample_weight parameter\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict_proba(self):\n",
    "        \"\"\"\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def learn_one(self):\n",
    "        \"\"\"\n",
    "            A wrapper for learning compatible with the river plataform, but \n",
    "            to using lazy classifiers (i. e. sklearn implementation)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def predict_one(self):\n",
    "        \"\"\"\n",
    "            A wapprer for predict compatible with the river plataform, but to\n",
    "            using lazy classifiers (i. e. sklearn implementation)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "HEEsbJX-0ebi",
    "outputId": "e4159401-039a-44d2-f745-714692cfdbc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest (with 100 trees) trained.\n",
      "Score within of training data =  1.0\n"
     ]
    }
   ],
   "source": [
    "X_train = X[:1000].values\n",
    "y_train = y[:1000].values\n",
    "X_test = X[1000:2000].values\n",
    "y_test = y[1000:2000].values\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=100,\n",
    "                                criterion='entropy',\n",
    "                                max_features='auto',\n",
    "                                bootstrap=True,\n",
    "                                oob_score=True,\n",
    "                                random_state=42,\n",
    "                                warm_start=True,\n",
    "                                n_jobs=-1\n",
    ")\n",
    "clf_rf = clf_rf.fit(X_train, y_train)\n",
    "print(\"\\nRandom Forest (with 100 trees) trained.\")\n",
    "print('Score within of training data = ', clf_rf.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exceuta o k tree \n",
    "ktree = KTreeClassifier(clf_rf, strategy=\"score\")\n",
    "ktree = ktree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Z9owcQEx0erl"
   },
   "outputs": [],
   "source": [
    "y_pred = ktree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "o-uOVL8h0fLD"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709877542212871"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ktree.get_score_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9709877542212871, 0.0031668865415189597)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ktree.get_score_train(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score, f1_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "def _get_metric(metric_type):\n",
    "    \"\"\"\n",
    "        Decode the metric function to use for evaluable classifiers\n",
    "    \"\"\"\n",
    "    if metric_type == 'accuracy_score':\n",
    "        return accuracy_score\n",
    "    elif metric_type == 'cohen_kappa_score':\n",
    "        return cohen_kappa_score\n",
    "    elif metric_type == 'f1_score':\n",
    "        return f1_score\n",
    "    elif metric_type == 'balanced_accuracy_score':\n",
    "        return balanced_accuracy_score\n",
    "    elif metric_type == 'precision_score':\n",
    "        return precision_score\n",
    "    elif metric_type == 'recall_score':\n",
    "        return recall_score\n",
    "    elif metric_type == 'roc_auc_score':\n",
    "        return roc_auc_score\n",
    "    else:\n",
    "        raise Exception('The parameter metric_type can not empty or unknow')\n",
    "        \n",
    "        \n",
    "metric = _get_metric('roc_auc_score')\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lizn81lAnQ1L",
    "outputId": "03d6f53d-9949-439b-eaa6-a48a72de2cb7"
   },
   "outputs": [],
   "source": [
    "random_seed_4 = 42\n",
    "\n",
    "est = DecisionTreeClassifier(criterion='entropy',\n",
    "                                max_features='auto',\n",
    "                                random_state=random_seed_4\n",
    ")\n",
    "est = est.fit(X[:1000], y[:1000])\n",
    "print(\"\\nDecision Tree (Single) trained.\")\n",
    "print('Score within of training data = ', est.score(X[:1000], y[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qGZ36UL_xEhz",
    "outputId": "c1f7cae2-b678-42fe-d1de-c5b3589a4b01"
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from os import cpu_count\n",
    "\n",
    "n_jobs = -1\n",
    "verbose = 0\n",
    "check_input = True\n",
    "sample_weight = None\n",
    "ci = check_input\n",
    "\n",
    "def _get_n_jobs():\n",
    "  return (cpu_count() if n_jobs == -1 else n_jobs)\n",
    "\n",
    "y_pred = Parallel(n_jobs=_get_n_jobs(),\n",
    "                          verbose=verbose, \n",
    "                          require='sharedmem'\n",
    "        )(delayed(est.predict)(Xi, ci) for Xi in np.array_split(X[1900:2000], _get_n_jobs()))\n",
    "\n",
    "y_pred = np.ravel(y_pred)\n",
    "#np.array_split(X[1000:2000], _get_n_jobs())\n",
    "print(y_pred.shape)\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mhoNpd4InCNE",
    "outputId": "3fc17050-f6eb-45d6-ff9b-925a71f73e96"
   },
   "outputs": [],
   "source": [
    "#accuracy_score(y[1900:2000], y_pred)\n",
    "metric = roc_auc_score\n",
    "dp = {'average': 'weighted', 'sample_weight': None, 'max_fpr': None, 'multi_class': 'ovr', 'labels': None}\n",
    "roc_auc_score(y[1900:2000], y_pred, **dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EmdjOm4_n3Ej",
    "outputId": "8534f5ce-6a1a-4d2e-9232-3225fdca7a6b"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(12, 20))\n",
    "plot_tree(clf_dt, max_depth=2, ax=axs)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IwrG5ExoVHx",
    "outputId": "95de3b5d-6752-4986-8149-c1a396bd679c"
   },
   "outputs": [],
   "source": [
    "clf_dt = clf_dt.fit(X[1000:2000], y[1000:2000])\n",
    "print(\"\\nDecision Tree (Single) trained.\")\n",
    "print('Score within of training data = ', clf_dt.score(X[:1000], y[:1000]))\n",
    "print('Score within of training data = ', clf_dt.score(X[1000:2000], y[1000:2000]))\n",
    "print('Score within of training data = ', clf_dt.score(X[2000:3000], y[2000:3000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "J28iLs4k3j4P",
    "outputId": "80eb9e71-199b-4f7f-d0d3-2a35a2dc2b1a"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(12, 20))\n",
    "plot_tree(clf_dt, max_depth=2, ax=axs)\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
